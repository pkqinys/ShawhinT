{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e691f1-e5d5-4aa4-a53e-6ddcdd23e18d",
   "metadata": {},
   "source": [
    "# Exploring Llama 3.2-Vision (locally) with Ollama\n",
    "\n",
    "Code authored by: Shaw Talebi\n",
    "\n",
    "[Blog link](https://towardsdatascience.com/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3)\n",
    "<br>\n",
    "[Video link](https://youtu.be/Ot2c5MKN_-w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004ada7-b590-45ae-87d8-6968558bd932",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353a71c8-f7c8-4e7d-93df-d3a55cd6b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'llama3.2-vision' # 7b\n",
    "model = 'gemma3:4b' # 4b\n",
    "# model = 'moondream:1.8b' # 1.7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b99af-7ca5-4628-9377-ddd0f5bf30c7",
   "metadata": {},
   "source": [
    "### pull model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "726a200d-921e-4a8e-a38b-4ac38a99ad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77b5ab-7cd6-4815-b12c-6f7c58dbb5aa",
   "metadata": {},
   "source": [
    "#### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f526327-a9b5-46a9-9008-e9cb65ed260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The image features a man sitting on a yellow stool or ottoman, wearing a black shirt and tan pants. He has his hands clasped together while smiling at the camera. The room appears to be a living space with various furniture items such as chairs, a couch, and a TV mounted on the wall in the background.\n",
      "\n",
      "There are also several potted plants placed around the room, adding greenery and life to the space. A chair can be seen near one of the potted plants, while another is located closer to the foreground. The man's position on the yellow stool or ottoman suggests that he might be in a casual setting where seating options are provided for guests or family members.\n",
      "\n",
      "Additionally, there is a keyboard placed nearby, possibly indicating that this living space could also serve as a workspace or an area for creative pursuits such as music production or writing.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'What is in this image?',\n",
    "        'images': ['images/shaw-sitting.jpeg']\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfabe75-b6e2-41f4-8538-371192135ae2",
   "metadata": {},
   "source": [
    "#### Image captioning - streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc29b737-6f86-4cee-a7c4-274ba2f10c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urns, plants, and a television are arranged on the wooden floor of a modern living room.\n"
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Can you write a caption for this image?',\n",
    "        'images': ['images/shaw-sitting.jpeg']\n",
    "    }],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d3738-a3c0-4d44-83b8-7f90ddce9cd6",
   "metadata": {},
   "source": [
    "#### Explaining memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1ac5d88-6438-4ff6-849b-a888c90d8790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The image features a cartoon character, likely Spongebob Squarepants, sitting on a bench in a sandy area. The character is holding a hammer and appears to be working on something or fixing an object nearby. Various objects are scattered around the scene, including a blue wrench, a yellow toolbox, and several other tools. A text overlay at the bottom of the image reads \"Trying to build with AI today....\""
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Can you explain this meme to me?',\n",
    "        'images': ['images/ai-meme.jpeg']\n",
    "    }],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73926c2f-826f-4a15-8a67-b5ca4e4b6d9e",
   "metadata": {},
   "source": [
    "#### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06e911d1-533d-48ba-923a-549544aba5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the text from the screenshot formatted in markdown:\n",
      "\n",
      "**5 AI Projects You Can Build This Weekend (with Python)**\n",
      "\n",
      "1.  Resume Optimization (Beginner)\n",
      "    *   Idea: build a tool that adapts your resume for a specific job description\n",
      "\n",
      "2.  YouTube Lecture Summarizer (Beginner)\n",
      "    *   Idea: build a tool that takes YouTube video link and summarizes it\n",
      "\n",
      "3.  Automatically Organizing PDFs (Intermediate)\n",
      "    *   Idea: build a tool to analyze the contents of each PDF and organize them into folders based on topics.\n",
      "\n",
      "4.  Multimodal Search (Intermediate)\n",
      "    *   Idea: Use multimodal embeddings to represent user queries, text knowledge, and images in single space\n",
      "\n",
      "5.  Desktop QA (Advanced)\n",
      "    *   Idea: Connect a multimodal knowledge base to a multimodal model like Llama-3.2-11B-Vision."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Can you transcribe the text from this screenshot in a markdown format?',\n",
    "        'images': ['images/5-ai-projects.jpeg']\n",
    "    }],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
